{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Classification Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will learn how to generate synthetic data and how to apply various built-in classifiers to classify the data. The goal of this lab is to introduce you to a subset of classifcation methods and which methods perform better depending on the structure of the data. We will also see where data visulaization and background information can come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's scikit-learn (or sklearn) is a Machine Learning library equipped with simple and efficient tools for data mining and data analysis. In general, a learning problem consists of a set of $n$ samples of data and then tries to predict properties of _unknown_ data.\n",
    "\n",
    "If each sample is **more** than a single number (aka multivariate), it is said to have several attributes or **features**. \n",
    "\n",
    "We will mainly focus on **supervised** learning, in which each input data has an associated label. If we have time, we will discuss unsupervised learning and what techniques are commonly used today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "X, y = make_classification(n_classes = 2, n_samples = 100, \\\n",
    "                           n_features = 2, n_redundant = 0, \\\n",
    "                           random_state = 1, n_clusters_per_class = 1)\n",
    "\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape) # add some noise to the data\n",
    "linearly_separable = (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*make_classification* generates a random classification problem where the number of class is user-specified. Later we will see *make_moons* and *make_circles*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure( figsize=(27,9) )\n",
    "color_map = plt.cm.RdBu #Red-Blue colormap\n",
    "cm_bright = ListedColormap(['#FF0000','#0000FF'])\n",
    "\n",
    "X = StandardScaler().fit_transform(X)# center and scale the data\n",
    "\n",
    "# Split the data to reserve some for model validation\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 # for plotting\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 # y-axis, not to be confused with y labels\n",
    "\n",
    "# Don't worry about the mesh grid now, we will use it to make pretty plots!\n",
    "h = 0.02 # the mesh step size\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "plt.title(\"(Make Classification) Input Data\", fontsize = 28)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "\n",
    "plt.axis([x_min, x_max, y_min, y_max])\n",
    "plt.grid(True)\n",
    "plt.xlabel('x', fontsize = 28), plt.ylabel('y', fontsize = 28)\n",
    "plt.tick_params(labelsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $K$-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we are going to consider is $k$-nearest neighbors. The idea behind the method is that the input consists of the $k$ closest training examples in the feature space. An object is classified by a **majority vote** of its neighbors, with the object being assignmed to the class most common among its **_k_** nearest neighbors, hence the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity! Get up!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "names = [\"Nearest Neighbors\"]\n",
    "#cl = [KNeighborsClassifier(3,weights = 'distance')] # Let's choose k=3\n",
    "cl = [KNeighborsClassifier(3)] # Let's choose k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl[0].fit(X_train,y_train)\n",
    "score = cl[0].score(X_test,y_test)\n",
    "\n",
    "# Plot the decision boundary for which we will assign a color to each class\n",
    "# concatonate vectorized grid and compute probability estimates for the data\n",
    "\n",
    "Z = cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] # with confidence bds\n",
    "#Z = cl[0].predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "plt.title('Decision Boundary', fontsize = 18)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the data...\n",
    "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "plt.title(names[0], fontsize = 20)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold='nan')\n",
    "#cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] # with confidence bds\n",
    "print(score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We see that the $k$-nearest neighbors algorithm was able to classify the unseen data with an accuracy of 90%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will take a look at Decision Tree Classification. A decision tree is a flowchart-like structure in which each internal **node** represents a \"test\" on an attribute (or feature), each **branch** represents the outcome of the test, and each **leaf** node represents a class label. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-12-638.jpg?cb=1444928106')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://help.prognoz.com/en/mergedProjects/Lib/img/decisiontree.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "names.append(\"Decision Tree\")\n",
    "cl.append(DecisionTreeClassifier(max_depth=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl[1].fit(X_train,y_train)\n",
    "score = cl[1].score(X_test,y_test)\n",
    "\n",
    "# Plot the decision boundary for which we will assign a color to each class\n",
    "# concatonate vectorized grid and compute probability estimates for the data\n",
    "\n",
    "#Z = cl[1].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] # with confidence bds\n",
    "Z = cl[1].predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "plt.title('Decision Boundary', fontsize = 18)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the data...\n",
    "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "plt.title(names[1], fontsize = 20)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better! A Decision Tree Classifier was able to predict the unseen data labels with an accuracy of over 93%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at how these algorithms perform on data with different structural relationships. As you may think, sklearn's *make_moons* can artificial generate data whose class labeling's form \"moons\" around each other. Similarily, *make_circles* generates data grouped with circular structure. \n",
    "\n",
    "The cells below provide a visual of the what these functions output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(noise = 0.3, random_state = 0, n_samples = 200)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "plt.title(\"Make Moons Dataset\", fontsize = 28)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "\n",
    "plt.axis([x_min, x_max, y_min, y_max])\n",
    "plt.grid(True)\n",
    "plt.xlabel('x', fontsize = 28), plt.ylabel('y', fontsize = 28)\n",
    "plt.tick_params(labelsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "# Fit make_moons dataset with k-Nearest Neighbors Classifier\n",
    "cl[0].fit(X_train,y_train)\n",
    "score = cl[0].score(X_test,y_test)\n",
    "\n",
    "Z = cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "ax.set_title(names[0], fontsize = 20)\n",
    "\n",
    "# Fit make_circles dataset with Decision Tree Classifier\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "cl[1].fit(X_train,y_train)\n",
    "score2 = cl[1].score(X_test,y_test)\n",
    "\n",
    "Z = cl[1].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax2.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "ax2.set_title(names[1], fontsize = 20)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"Nearest Neighbors Score:\",score)\n",
    "print(\"Decision Tree Score:\",score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(noise = 0.2, factor = 0.5, random_state = 1,n_samples = 200)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "    \n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "plt.title(\"Make Circles Dataset\", fontsize = 28)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "\n",
    "plt.axis([x_min, x_max, y_min, y_max])\n",
    "plt.grid(True)\n",
    "plt.xlabel('x', fontsize = 28), plt.ylabel('y', fontsize = 28)\n",
    "plt.tick_params(labelsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "# Fit make_circles dataset with k-Nearest Neighbors Classifier\n",
    "cl[0].fit(X_train,y_train)\n",
    "score = cl[0].score(X_test,y_test)\n",
    "\n",
    "Z = cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "ax.set_title(names[0], fontsize = 20)\n",
    "\n",
    "# Fit make_circles dataset with Decision Tree Classifier\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "cl[1].fit(X_train,y_train)\n",
    "score2 = cl[1].score(X_test,y_test)\n",
    "\n",
    "Z = cl[1].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax2.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
    "ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
    "ax2.set_title(names[1], fontsize = 20)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"Nearest Neighbors Score:\",score)\n",
    "print(\"Decision Tree Score:\",score2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
