{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Logistic Regression and Text Models\n",
    "***\n",
    "\n",
    "<img src=\"figs/logregwordcloud.png\",width=1000,height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Logistic Regression for 2D Continuous Features \n",
    "***\n",
    "\n",
    "In the video lecture you saw some examples of using logistic regression to do binary classification on text data (SPAM vs HAM) and on 1D continuous data.  In this problem we'll look at logistic regression for 2D continuous data. The data we'll use are <a href=\"https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg\">sepal</a> measurements from the ubiquitous *iris* dataset.  \n",
    "\n",
    "\n",
    "<!---\n",
    "<img style=\"float:left; width:450px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg\",width=300,height=50>\n",
    "-->\n",
    "\n",
    "<img style=\"float:left; width:450px\" src=\"http://www.twofrog.com/images/iris38a.jpg\",width=300,height=50>\n",
    "\n",
    "<!---\n",
    "<img style=\"float:right; width:490px\" src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\",width=300,height=50>\n",
    "-->\n",
    "\n",
    "<img style=\"float:right; width:490px\" src=\"http://blazingstargardens.com/wp-content/uploads/2016/02/Iris-versicolor-Blue-Flag-Iris1.jpg\",width=300,height=62>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two features of our model will be the **sepal length** and **sepal width**.  Execute the following cell to see a plot of the data. The blue points correspond to the sepal measurements of the Iris Setosa (left) and the red points correspond to the sepal measurements of the Iris Versicolour (right). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_train = iris.data[iris.target != 2, :2] # first two features and\n",
    "y_train = iris.target[iris.target != 2]   # first two labels only \n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=100, alpha=0.9, c=[mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y_train])\n",
    "plt.xlabel('sepal length', fontsize=16)\n",
    "plt.ylabel('sepal width', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a logistic regression model of the form \n",
    "\n",
    "$$\n",
    "p(y = 1 ~|~ {\\bf x}; {\\bf w}) = \\frac{1}{1 + \\textrm{exp}[-(w_0 + w_1x_1 + w_2x_2)]}\n",
    "$$\n",
    "\n",
    "using **sklearn**'s logistic regression classifier as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # import from sklearn \n",
    "logreg = LogisticRegression()                       # initialize classifier \n",
    "logreg.fit(X_train, y_train);                       # train on training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Determine the parameters ${\\bf w}$ fit by the model.  It might be helpful to consult the documentation for the classifier on the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">sklearn website</a>. **Hint**: The classifier stores the coefficients and bias term separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: In general, what does the Logistic Regression decision boundary look like for data with two features?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Modify the code below to plot the decision boundary along with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-e189b8aedadc>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-e189b8aedadc>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    x2 = #TODO\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=100, c=[mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y_train])\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "x_min, x_max = np.min(X_train[:,0])-0.1, np.max(X_train[:,0])+0.1\n",
    "y_min, y_max = np.min(X_train[:,1])-0.1, np.max(X_train[:,1])+0.1\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "x1 = np.linspace(x_min, x_max, 100)\n",
    "w0 = logreg.intercept_\n",
    "w1 = logreg.coef_[0][0]\n",
    "w2 = logreg.coef_[0][1]\n",
    "x2 = #TODO \n",
    "plt.plot(x1, x2, color=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: The Bag-of-Words Text Model \n",
    "***\n",
    "\n",
    "The remainder of today's exercise will consider the problem of predicting the semantics of text.  In particular, later we'll look at predicting whether movie reviews are positive or negative just based on their text. \n",
    "\n",
    "Before we can utilize text as features in a learning model, we need a concise mathematical way to represent things like words, phrases, sentences, etc.  The most common text models are based on the so-called <a href=\"https://en.wikipedia.org/wiki/Vector_space_model\">Vector Space Model</a> (VSM) where individual words in a document are associated with entries of a vector: \n",
    "\n",
    "$$\n",
    "\\textrm{\"The sky is blue\"} \\quad \\Rightarrow \\quad \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\ \n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The first step in creating a VSM is to define a vocabulary, $V$, of words that you will include in your model.  This vocabulary can be determined by looking at all (or most) of the words in the training set, or even by including a fixed vocabulary based on the english language.  A vector representation of a document like a movie review is then a vector with length $|V|$ where each entry in the vector maps uniquely to a word in the vocabulary. A vector encoding of a document would then be a vector that is nonzero in positions corresponding to words present in the document and zero everywhere else.  How you fill in the nonzero entries depends on the model you're using.  Two simple conventions are the **Bag-of-Words** model and the **binary** model.  \n",
    "\n",
    "In the binary model we simply set an entry of the vector to $1$ if the associate word appears at least once in the document.  In the more common Bag-of-Words model we set an entry of the vector equal to the frequency with which the word appears in the document. Let's see if we can come up with a simple implementation of the Bag-of-Words model in Python, and then later we'll see how sklearn can do the heavy lifting for us. \n",
    "\n",
    "Consider a training set containing three documents, specified as follows \n",
    "\n",
    "$\\texttt{Training Set}:$\n",
    "\n",
    "$\\texttt{d1}: \\texttt{new york times}$\n",
    "\n",
    "$\\texttt{d2}: \\texttt{new york post}$\n",
    "\n",
    "$\\texttt{d3}: \\texttt{los angeles times}$\n",
    "\n",
    "\n",
    "First we'll define the vocabulary based on the words in the test set.  It is $V = \\{ \\texttt{angeles}, \\texttt{los}, \\texttt{new}, \\texttt{post}, \\texttt{times}, \\texttt{york}\\}$. \n",
    "\n",
    "\n",
    "We need to define an association between the particular words in the vocabulary and the specific entries in our vectors.  Let's define this association in the order that we've listed them above.  We can store this mapping as a Python dictionary as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = {\"angeles\": 0, \"los\": 1, \"new\": 2, \"post\": 3, \"times\": 4, \"york\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also store the documents in a list as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [\"new york times\", \"new york post\", \"los angeles times\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with sklearn conventions, we'll encode the documents as *row-vectors* stored in a matrix.  In this case, each row of the matrix corresponds to a document, and each column corresponds to a term in the vocabulary.  For our example this gives us a matrix $M$ of shape $3 \\times 6$.  The $(d,t)$-entry in $M$ is then the number of times the term $t$ appears in document $d$\n",
    "\n",
    "**Q**: Your first task is to write some simple Python code to construct the *term-frequency* matrix $M$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-dd73337402b6>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-dd73337402b6>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print M\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "M = np.zeros((len(D),len(V)))\n",
    "\n",
    "for ii, doc in enumerate(D): \n",
    "    for term in doc.split(): \n",
    "        #TODO \n",
    "        \n",
    "print M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully your code returns the matrix \n",
    "\n",
    "$$M = \n",
    "\\left[\n",
    "\\begin{array}{ccccccc}\n",
    "0 & 0 & 1 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 1 \\\\\n",
    "1 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "\\right]$$.  \n",
    "\n",
    "Note that the entry in the (2,0) position is $1$ because the first word (angeles) appears once in the third document. \n",
    "\n",
    "OK, let's see how we can construct the same term-frequency matrix in sklearn.  We will use something called the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a> to accomplish this. Let's see some code and then we'll explain how it functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    # import CountVectorizer \n",
    "vectorizer = CountVectorizer()                                 # initialize the vectorizer\n",
    "X = vectorizer.fit_transform(D)                                # fit to training data and transform to matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\texttt{fit_transform}$ method actually does two things.  It fits the model to the training data by building a vocabulary.  It then transforms the text in $D$ into matrix form.  \n",
    "\n",
    "If we wish to see the vocabulary you can do it like so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the same vocabulary and indexing that we definfed ourselves.  Hopefully that means we'll get the same term-frequency matrix.  We can print $X$ and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, they're the same!  Notice that we had to convert $X$ to a dense matrix for printing.  This is because CountVectorizer actually returns a sparse matrix.  This is a very good thing since most vectors in a text model will be **extremely** sparse, since most documents will only contain a handful of words from the vocabulary. \n",
    "\n",
    "OK, let's see how we can use the CountVectorizer to transform the test documents into their own term-frequency matrix.  Removing the stop words we have \n",
    "\n",
    "OK, now suppose that we have a query document not included in the training set that we want to vectorize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = [\"new york new tribune\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already fit the CountVectorizer to the training set, so all we need to do is transform the test set documents into a term-frequency vector using the same conventions.  Since we've already fit the model, we do the transformation with the $\\texttt{transform}$ method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = vectorizer.transform(d4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print it and see what it looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x4.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the query document included the word $\\texttt{new}$ twice, which corresponds to the entry in the $(0,2)$-position. \n",
    "\n",
    "**Q**: What's missing from $x4$ that we might expect to see from the query document? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Problem 3: Term Frequency - Inverse Document Frequency \n",
    "***\n",
    "\n",
    "The Bag-of-Words model for text classification is very popular, but let's see if we can do better.  Currently we're weighting every word in the corpus by it's frequency.  It turns out that in text classification there are often features that are not particularly useful predictors for the document class, either because they are too common or too uncommon.  **Stop-words** are extremely common, low-information words like \"a\", \"the\", \"as\", etc.  Removing these from documents is typically the first thing done in peparing data for document classification. \n",
    "\n",
    "**Q**: Can you think of a situation where it might be useful to keep stop words in the corpus? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other words that tend to be uninformative predictors are words that appear very very rarely.  In particular, if they do not appear frequently enough in the training data then it is difficult for a classification algorithm to weight them heavily in the classification process. \n",
    "\n",
    "In general, the words that tend to be useful predictors are the words that appear frequently, but not too frequently.  Consider the following frequency graph for a corpus. \n",
    "\n",
    "<img src=\"figs/feat_freq.png\",width=400,height=50>\n",
    "\n",
    "The features in column A appear too frequently to be very useful, and the features in column C appear too rarely. One first-pass method of feature selection in text classification would be to discard the words from columns A and C, and build a classifier with only features from column B.\n",
    "\n",
    "Another common model for identifying the useful terms in a document is the Term Frequency - Inverse Document Frequency (tf-idf) model.  Here we won't throw away any terms, but we'll replace their Bag-of-Words frequency counts with tf-idf scores which we describe below. \n",
    "\n",
    "The tf-idf score is the product of two statistics, *term frequency* and *inverse document frequency*\n",
    "\n",
    "\n",
    "$$\\texttt{tfidf(d,t)} = \\texttt{tf(d,t)} \\times \\texttt{idf(t)}$$\n",
    "\n",
    "The term frequency $\\texttt{tf(d,t)}$ is a measure of the frequency with which term $t$ appears in document $d$.  The inverse document frequency $\\texttt{idf(t)}$ is a measure of how much information the word provides, that is, whether the term is common or rare across all documents.  By multiplying the two quantities together, we obtain a representation of term $t$ in document $d$ that weighs how common the term is in the document with how common the word is in the entire corpus. You can imagine that the words that get the highest associated values are terms that appear many times in a small number of documents. \n",
    "\n",
    "\n",
    "There are many ways to compute the composite terms $\\texttt{tf}$ and $\\texttt{idf}$.  For simplicity, we'll define $\\texttt{tf(d,t)}$ to be the number of times term $t$ appears in document $d$ (i.e., Bag-of-Words). We will define the inverse document frequency as follows: \n",
    "\n",
    "$$\n",
    "\\texttt{idf(t)} = \\ln ~ \\frac{\\textrm{total # documents}}{\\textrm{1 + # documents with term }t}\n",
    " = \\ln ~ \\frac{|D|}{|d: ~ t \\in d |}\n",
    "$$\n",
    "\n",
    "Note that we could have a potential problem if a term comes up that is not in any of the training documents, resulting  in a divide by zero.  This might happen if you use a canned vocabulary instead of constructing one from the training documents.  To guard against this, many implementations will use add-one smoothing in the denominator (this is what sklearn does). \n",
    "\n",
    "$$\n",
    "\\texttt{idf(t)} = \\ln ~ \\frac{\\textrm{total # documents}}{\\textrm{1 + # documents with term }t}\n",
    " = \\ln ~ \\frac{|D|}{1 + |d: ~ t \\in d |}\n",
    "$$\n",
    "\n",
    "**Q**: Compute $\\texttt{idf(t)}$ (without smoothing) for each of the terms in the training documents from the previous problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Compute the td-ifd matrix for the training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you got something like the following: \n",
    "\n",
    "$$\n",
    "X_{tfidf} = \n",
    "\\left[\n",
    "\\begin{array}{ccccccccc}\n",
    "0.          & 0.         &  0.40546511 &  0.         &  0.40546511 &  0.40546511 \\\\\n",
    "0.          & 0.         &  0.40546511 &  1.09861229 &  0.         &  0.40546511 \\\\\n",
    "1.09861229  & 1.09861229 &  0.         &  0.         &  0.40546511 &  0.        \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The final step in any VSM method is the normalization of the vectors.  This is done so that very long documents to not completely overpower the small and medium length documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_norms = np.array([np.linalg.norm(row) for row in Xtfidf])\n",
    "X_tfidf_n = np.dot(np.diag(1./row_norms), Xtfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_tfidf_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get when we use sklearn.  Sklearn has a vectorizer called TfidfVectorizer which is similar to CountVectorizer, but it computes tf-idf scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "Y = tfidf.fit_transform(D)\n",
    "print Y.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are not quite the same, becuase sklearn's implementation of tf-idf uses the add-one smoothing in the denominator for idf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Problem 4: Classifying Semantics in Movie Reviews\n",
    "***\n",
    "> The data for this problem was taken from the <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\">Bag of Words Meets Bag of Popcorn</a> Kaggle competition\n",
    "\n",
    "In this problem you will use the text from movie reviews to predict whether the reviewer felt positively or negatively about the movie using Bag-of-Words and tf-idf. I've partially cleaned the data and stored it in files called $\\texttt{labeledTrainData.tsv}$ and $\\texttt{labeledTestData.tsv}$ in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "def read_and_clean_data(fname, remove_stops=True):\n",
    "    \n",
    "    with open('data/stopwords.txt', 'rt') as f:\n",
    "        stops = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    with open(fname,'rt') as tsvin:\n",
    "        reader = csv.reader(tsvin, delimiter='\\t')\n",
    "        labels = []; text = [] \n",
    "        for ii, row in enumerate(reader):\n",
    "            labels.append(int(row[0]))\n",
    "            words = row[1].lower().split()\n",
    "            words = [w for w in words if not w in stops] if remove_stops else words \n",
    "            text.append(\" \".join(words))\n",
    "    \n",
    "    return text, labels\n",
    "\n",
    "text_train, labels_train = read_and_clean_data('data/labeledTrainData.tsv', remove_stops=True)\n",
    "text_test, labels_test = read_and_clean_data('data/labeledTestData.tsv', remove_stops=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current parameters are set to not remove stop words from the text so that it's a bit easier to explore. \n",
    "\n",
    " Look at a few of the reviews stored in $\\texttt{text_train}$ as well as their associated labels in $\\texttt{labels_train}$.  Can you figure out which label refers to a positive review and which refers to a negative review? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first review is labeled $1$ and has the following text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth review is labeled $0$ and has the following text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully it's obvious that label 1 corresponds to positive reviews and label 0 to negative reviews! \n",
    "\n",
    "OK, the first thing we'll do is train a logistic regression classifier using the Bag-of-Words model, and see what kind of accuracy we can get.  To get started, we need to vectorize the text into mathematical features that we can use.  We'll use CountVectorizer to do the job. (Before starting, I'm going to reload the data and remove the stop words this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, labels_train = read_and_clean_data('data/labeledTrainData.tsv', remove_stops=True)\n",
    "text_test, labels_test = read_and_clean_data('data/labeledTestData.tsv', remove_stops=True)\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "X_bw_train = cvec.fit_transform(text_train)\n",
    "y_train = np.array(labels_train)\n",
    "X_bw_test  = cvec.transform(text_test)\n",
    "y_test  = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How many different words are in the vocabulary? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we'll train a logistic regression classifier on the training set, and test the accuracy on the test set.  To do this we'll need to load some kind of accuracy metric from sklearn.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "bwLR = LogisticRegression()\n",
    "bwLR.fit(X_bw_train, y_train)\n",
    "pred_bwLR = bwLR.predict(X_bw_test)\n",
    "\n",
    "print \"Logistic Regression accuracy with Bag-of-Words: \", accuracy_score(y_test, pred_bwLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we got an accuracy of around 81% using Bag-of-Words.  Now lets do the same tests but this time with tf-idf features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "X_tf_train = tvec.fit_transform(text_train)\n",
    "X_tf_test  = tvec.transform(text_test)\n",
    "\n",
    "tfLR = LogisticRegression()\n",
    "tfLR.fit(X_tf_train, y_train)\n",
    "pred_tfLR = tfLR.predict(X_tf_test)\n",
    "\n",
    "print \"Logistic Regression accuracy with tf-idf: \", accuracy_score(y_test, pred_tfLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOOHOO**!  With tf-idf features we got around 85% accuracy, which is a 4% improvement. (If you're scoffing at this, wait until you get some more experience working with real-world data.  4% improvement is pretty awesome).  \n",
    "\n",
    "**Q**: Which words are the strongest predictors for a positive review and which words are the strongest predictors for negative reviews? I'm not going to give you the answer to this one because it's the same question we'll ask on the next homework assignment. But if you figure this out you'll have a great head start! \n",
    "\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "## Notebook Solutions\n",
    "***\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Logistic Regression for 2D Continuous Features \n",
    "***\n",
    "\n",
    "In the video lecture you saw some examples of using logistic regression to do binary classification on text data (SPAM vs HAM) and on 1D continuous data.  In this problem we'll look at logistic regression for 2D continuous data. The data we'll use are <a href=\"https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg\">sepal</a> measurements from the ubiquitous *iris* dataset.  \n",
    "\n",
    "\n",
    "<!---\n",
    "<img style=\"float:left; width:450px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg\",width=300,height=50>\n",
    "-->\n",
    "\n",
    "<img style=\"float:left; width:450px\" src=\"http://www.twofrog.com/images/iris38a.jpg\",width=300,height=50>\n",
    "\n",
    "<!---\n",
    "<img style=\"float:right; width:490px\" src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\",width=300,height=50>\n",
    "-->\n",
    "\n",
    "<img style=\"float:right; width:490px\" src=\"http://blazingstargardens.com/wp-content/uploads/2016/02/Iris-versicolor-Blue-Flag-Iris1.jpg\",width=300,height=62>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two features of our model will be the **sepal length** and **sepal width**.  Execute the following cell to see a plot of the data. The blue points correspond to the sepal measurements of the Iris Setosa (left) and the red points correspond to the sepal measurements of the Iris Versicolour (right). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_train = iris.data[iris.target != 2, :2] # first two features and\n",
    "y_train = iris.target[iris.target != 2]   # first two labels only \n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=100, alpha=0.9, c=[mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y_train])\n",
    "plt.xlabel('sepal length', fontsize=16)\n",
    "plt.ylabel('sepal width', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a logistic regression model of the form \n",
    "\n",
    "$$\n",
    "p(y = 1 ~|~ {\\bf x}; {\\bf w}) = \\frac{1}{1 + \\textrm{exp}[-(w_0 + w_1x_1 + w_2x_2)]}\n",
    "$$\n",
    "\n",
    "using **sklearn**'s logistic regression classifier as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # import from sklearn \n",
    "logreg = LogisticRegression()                       # initialize classifier \n",
    "logreg.fit(X_train, y_train);                       # train on training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Determine the parameters ${\\bf w}$ fit by the model.  It might be helpful to consult the documentation for the classifier on the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">sklearn website</a>. **Hint**: The classifier stores the coefficients and bias term separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: The bias term is stored in logreg.intercept\\_ . The remaining coefficients are stored in logreg.coef\\_ . For this problem we have \n",
    "\n",
    "$$\n",
    "w_0 =-0.599, \\quad w_1 = 2.217, \\quad \\textrm{and} \\quad w_2 =  -3.692\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: In general, what does the Logistic Regression decision boundary look like for data with two features?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: The decision boundary for Logistic Regresion for data with two features is a line.  To see this, remember that the decision boundary is made up of $(x_1, x_2)$ points such that $\\textrm{sigm}({\\bf w}^T{\\bf x}) = 0.5$.  We then have \n",
    "\n",
    "$$\n",
    "\\frac{1}{1 + \\textrm{exp}[-(w_0 + w_1x_1 + w_2x_2)]} = \\frac{1}{2} ~~\\Rightarrow ~~ w_0 + w_1x_1 + w_2x_2 = 0 ~~\\Rightarrow~~ x_2 = -\\frac{w_1}{w_2}x_1 - \\frac{w_0}{w_2}\n",
    "$$\n",
    "\n",
    "So the decision boundary is a line with slope $-w_1/w_2$ and intercept $-w_0/w_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Modify the code below to plot the decision boundary along with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=100, c=[mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y_train])\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "x_min, x_max = np.min(X_train[:,0])-0.1, np.max(X_train[:,0])+0.1\n",
    "y_min, y_max = np.min(X_train[:,1])-0.1, np.max(X_train[:,1])+0.1\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "x1 = np.linspace(x_min, x_max, 100)\n",
    "w0 = logreg.intercept_\n",
    "w1 = logreg.coef_[0][0]\n",
    "w2 = logreg.coef_[0][1]\n",
    "x2 = -(w0/w2) - (w1/w2)*x1 #TODO \n",
    "plt.plot(x1, x2, color=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: The Bag-of-Words Text Model \n",
    "***\n",
    "\n",
    "The remainder of today's exercise will consider the problem of predicting the semantics of text.  In particular, later we'll look at predicting whether movie reviews are positive or negative just based on their text. \n",
    "\n",
    "Before we can utilize text as features in a learning model, we need a concise mathematical way to represent things like words, phrases, sentences, etc.  The most common text models are based on the so-called <a href=\"https://en.wikipedia.org/wiki/Vector_space_model\">Vector Space Model</a> (VSM) where individual words in a document are associated with entries of a vector: \n",
    "\n",
    "$$\n",
    "\\textrm{\"The sky is blue\"} \\quad \\Rightarrow \\quad \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\ \n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The first step in creating a VSM is to define a vocabulary, $V$, of words that you will include in your model.  This vocabulary can be determined by looking at all (or most) of the words in the training set, or even by including a fixed vocabulary based on the english language.  A vector representation of a document like a movie review is then a vector with length $|V|$ where each entry in the vector maps uniquely to a word in the vocabulary. A vector encoding of a document would then be a vector that is nonzero in positions corresponding to words present in the document and zero everywhere else.  How you fill in the nonzero entries depends on the model you're using.  Two simple conventions are the **Bag-of-Words** model and the **binary** model.  \n",
    "\n",
    "In the binary model we simply set an entry of the vector to $1$ if the associate word appears at least once in the document.  In the more common Bag-of-Words model we set an entry of the vector equal to the frequency with which the word appears in the document. Let's see if we can come up with a simple implementation of the Bag-of-Words model in Python, and then later we'll see how sklearn can do the heavy lifting for us. \n",
    "\n",
    "Consider a training set containing three documents, specified as follows \n",
    "\n",
    "$\\texttt{Training Set}:$\n",
    "\n",
    "$\\texttt{d1}: \\texttt{new york times}$\n",
    "\n",
    "$\\texttt{d2}: \\texttt{new york post}$\n",
    "\n",
    "$\\texttt{d3}: \\texttt{los angeles times}$\n",
    "\n",
    "\n",
    "First we'll define the vocabulary based on the words in the test set.  It is $V = \\{ \\texttt{angeles}, \\texttt{los}, \\texttt{new}, \\texttt{post}, \\texttt{times}, \\texttt{york}\\}$. \n",
    "\n",
    "\n",
    "We need to define an association between the particular words in the vocabulary and the specific entries in our vectors.  Let's define this association in the order that we've listed them above.  We can store this mapping as a Python dictionary as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = {\"angeles\": 0, \"los\": 1, \"new\": 2, \"post\": 3, \"times\": 4, \"york\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also store the documents in a list as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [\"new york times\", \"new york post\", \"los angeles times\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with sklearn conventions, we'll encode the documents as *row-vectors* stored in a matrix.  In this case, each row of the matrix corresponds to a document, and each column corresponds to a term in the vocabulary.  For our example this gives us a matrix $M$ of shape $3 \\times 6$.  The $(d,t)$-entry in $M$ is then the number of times the term $t$ appears in document $d$\n",
    "\n",
    "**Q**: Your first task is to write some simple Python code to construct the *term-frequency* matrix $M$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.zeros((len(D),len(V)))\n",
    "\n",
    "for ii, doc in enumerate(D): \n",
    "    for term in doc.split(): \n",
    "        M[ii, V[term]] += 1\n",
    "        \n",
    "print M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully your code returns the matrix \n",
    "\n",
    "$$M = \n",
    "\\left[\n",
    "\\begin{array}{ccccccc}\n",
    "0 & 0 & 1 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 1 \\\\\n",
    "1 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "\\right]$$.  \n",
    "\n",
    "Note that the entry in the (2,0) position is $1$ because the first word (angeles) appears once in the third document. \n",
    "\n",
    "OK, let's see how we can construct the same term-frequency matrix in sklearn.  We will use something called the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a> to accomplish this. Let's see some code and then we'll explain how it functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    # import CountVectorizer \n",
    "vectorizer = CountVectorizer()                                 # initialize the vectorizer\n",
    "X = vectorizer.fit_transform(D)                                # fit to training data and transform to matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\texttt{fit_transform}$ method actually does two things.  It fits the model to the training data by building a vocabulary.  It then transforms the text in $D$ into matrix form.  \n",
    "\n",
    "If we wish to see the vocabulary you can do it like so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the same vocabulary and indexing that we definfed ourselves.  Hopefully that means we'll get the same term-frequency matrix.  We can print $X$ and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, they're the same!  Notice that we had to convert $X$ to a dense matrix for printing.  This is because CountVectorizer actually returns a sparse matrix.  This is a very good thing since most vectors in a text model will be **extremely** sparse, since most documents will only contain a handful of words from the vocabulary. \n",
    "\n",
    "\n",
    "OK, now suppose that we have a query document not included in the training set that we want to vectorize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = [\"new york new tribune\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already fit the CountVectorizer to the training set, so all we need to do is transform the test set documents into a term-frequency vector using the same conventions.  Since we've already fit the model, we do the transformation with the $\\texttt{transform}$ method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = vectorizer.transform(d4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print it and see what it looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x4.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the query document included the word $\\texttt{new}$ twice, which corresponds to the entry in the $(0,2)$-position. \n",
    "\n",
    "**Q**: What's missing from $x4$ that we might expect to see from the query document? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: The word $\\texttt{tribune}$ do not appear in vector $x4$ at all.  This is because it did not occur in the training set, which means it is not present in the VSM vocabulary.  This should not bother us too much.  Most reasonable text data sets will have most of the important words present in the training set and thus in the vocabulary.  On the other hand, the throw-away words that are present only in the test set are probably useless anyway, since the learning model is trained based on the text in the training set, and thus won't be able to do anything intelligent with words the model hasn't seen yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Problem 3: Term Frequency - Inverse Document Frequency \n",
    "***\n",
    "\n",
    "The Bag-of-Words model for text classification is very popular, but let's see if we can do better.  Currently we're weighting every word in the corpus by it's frequency.  It turns out that in text classification there are often features that are not particularly useful predictors for the document class, either because they are too common or too uncommon.  **Stop-words** are extremely common, low-information words like \"a\", \"the\", \"as\", etc.  Removing these from documents is typically the first thing done in peparing data for document classification. \n",
    "\n",
    "**Q**: Can you think of a situation where it might be useful to keep stop words in the corpus? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: If you plan to use bi-grams or tri-grams as features.  Bi-grams are pairs of words that appear side-by-side in a document, e.g. \"he went\", \"went to\", \"to the\", \"the store\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other words that tend to be uninformative predictors are words that appear very very rarely.  In particular, if they do not appear frequently enough in the training data then it is difficult for a classification algorithm to weight them heavily in the classification process. \n",
    "\n",
    "In general, the words that tend to be useful predictors are the words that appear frequently, but not too frequently.  Consider the following frequency graph for a corpus. \n",
    "\n",
    "<img src=\"figs/feat_freq.png\",width=400,height=50>\n",
    "\n",
    "The features in column A appear too frequently to be very useful, and the features in column C appear too rarely. One first-pass method of feature selection in text classification would be to discard the words from columns A and C, and build a classifier with only features from column B.\n",
    "\n",
    "Another common model for identifying the useful terms in a document is the Term Frequency - Inverse Document Frequency (tf-idf) model.  Here we won't throw away any terms, but we'll replace their Bag-of-Words frequency counts with tf-idf scores which we describe below. \n",
    "\n",
    "The tf-idf score is the product of two statistics, *term frequency* and *inverse document frequency*\n",
    "\n",
    "\n",
    "$$\\texttt{tfidf(d,t)} = \\texttt{tf(d,t)} \\times \\texttt{idf(t)}$$\n",
    "\n",
    "The term frequency $\\texttt{tf(d,t)}$ is a measure of the frequency with which term $t$ appears in document $d$.  The inverse document frequency $\\texttt{idf(t)}$ is a measure of how much information the word provides, that is, whether the term is common or rare across all documents.  By multiplying the two quantities together, we obtain a representation of term $t$ in document $d$ that weighs how common the term is in the document with how common the word is in the entire corpus. You can imagine that the words that get the highest associated values are terms that appear many times in a small number of documents. \n",
    "\n",
    "\n",
    "There are many ways to compute the composite terms $\\texttt{tf}$ and $\\texttt{idf}$.  For simplicity, we'll define $\\texttt{tf(d,t)}$ to be the number of times term $t$ appears in document $d$ (i.e., Bag-of-Words). We will define the inverse document frequency as follows: \n",
    "\n",
    "$$\n",
    "\\texttt{idf(t)} = \\ln ~ \\frac{\\textrm{total # documents}}{\\textrm{# documents with term }t}\n",
    " = \\ln ~ \\frac{|D|}{|d: ~ t \\in d |}\n",
    "$$\n",
    "\n",
    "Note that we could have a potential problem if a term comes up that is not in any of the training documents, resulting  in a divide by zero.  This might happen if you use a canned vocabulary instead of constructing one from the training documents.  To guard against this, many implementations will use add-one smoothing in the denominator (this is what sklearn does). \n",
    "\n",
    "$$\n",
    "\\texttt{idf(t)} = \\ln ~ \\frac{\\textrm{total # documents}}{\\textrm{1 + # documents with term }t}\n",
    " = \\ln ~ \\frac{|D|}{1 + |d: ~ t \\in d |}\n",
    "$$\n",
    "\n",
    "**Q**: Compute $\\texttt{idf(t)}$ (without smoothing) for each of the terms in the training documents from the previous problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: \n",
    "\n",
    "$\n",
    "\\texttt{idf}(\\texttt{angeles}) = \\ln ~ \\frac{3}{1} = \\ln ~ \\frac{3}{1} = 1.10\n",
    "$\n",
    "\n",
    "$\n",
    "\\texttt{idf}(\\texttt{los}) = \\ln ~ \\frac{3}{1} = \\ln ~ \\frac{3}{1} = 1.10\n",
    "$\n",
    "\n",
    "$\n",
    "\\texttt{idf}(\\texttt{new}) = \\ln ~ \\frac{3}{2} = \\ln ~ \\frac{3}{2} = 0.41\n",
    "$\n",
    "\n",
    "$\n",
    "\\texttt{idf}(\\texttt{post}) = \\ln ~ \\frac{3}{1} = \\ln ~ \\frac{3}{1} = 1.10\n",
    "$\n",
    "\n",
    "$\n",
    "\\texttt{idf}(\\texttt{times}) = \\ln ~ \\frac{3}{2} = \\ln ~ \\frac{3}{2} = 0.41\n",
    "$\n",
    "\n",
    "$\n",
    "\\texttt{idf}(\\texttt{york}) = \\ln ~ \\frac{3}{2} = \\ln ~ \\frac{3}{2} = 0.41\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Compute the td-ifd matrix for the training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: There are several ways to do this.  One way would be to multiply the term-frequency matrix on the right with a diagonal matrix with the idf-values on the main diagonal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.array([np.log(3), np.log(3), np.log(3./2), np.log(3), np.log(3./2), np.log(3./2)])\n",
    "Xtfidf = np.dot(X.todense(), np.diag(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Xtfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you got something like the following: \n",
    "\n",
    "$$\n",
    "X_{tfidf} = \n",
    "\\left[\n",
    "\\begin{array}{ccccccccc}\n",
    "0.          & 0.         &  0.40546511 &  0.         &  0.40546511 &  0.40546511 \\\\\n",
    "0.          & 0.         &  0.40546511 &  1.09861229 &  0.         &  0.40546511 \\\\\n",
    "1.09861229  & 1.09861229 &  0.         &  0.         &  0.40546511 &  0.        \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The final step in any VSM method is the normalization of the vectors.  This is done so that very long documents to not completely overpower the small and medium length documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_norms = np.array([np.linalg.norm(row) for row in Xtfidf])\n",
    "X_tfidf_n = np.dot(np.diag(1./row_norms), Xtfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_tfidf_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get when we use sklearn.  Sklearn has a vectorizer called TfidfVectorizer which is similar to CountVectorizer, but it computes tf-idf scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "Y = tfidf.fit_transform(D)\n",
    "print Y.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are not quite the same, becuase sklearn's implementation of tf-idf uses the add-one smoothing in the denominator for idf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Problem 4: Classifying Semantics in Movie Reviews\n",
    "***\n",
    "> The data for this problem was taken from the <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\">Bag of Words Meets Bag of Popcorn</a> Kaggle competition\n",
    "\n",
    "In this problem you will use the text from movie reviews to predict whether the reviewer felt positively or negatively about the movie using Bag-of-Words and tf-idf. I've partially cleaned the data and stored it in files called $\\texttt{labeledTrainData.tsv}$ and $\\texttt{labeledTestData.tsv}$ in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "def read_and_clean_data(fname, remove_stops=True):\n",
    "    \n",
    "    with open('data/stopwords.txt', 'r') as f:\n",
    "        stops = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    with open(fname,'rb') as tsvin:\n",
    "        reader = csv.reader(tsvin, delimiter='\\t')\n",
    "        labels = []; text = [] \n",
    "        for ii, row in enumerate(reader):\n",
    "            labels.append(int(row[0]))\n",
    "            words = row[1].lower().split()\n",
    "            words = [w for w in words if not w in stops] if remove_stops else words \n",
    "            text.append(\" \".join(words))\n",
    "    \n",
    "    return text, labels\n",
    "\n",
    "text_train, labels_train = read_and_clean_data('data/labeledTrainData.tsv', remove_stops=False)\n",
    "text_test, labels_test = read_and_clean_data('data/labeledTestData.tsv', remove_stops=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current parameters are set to not remove stop words from the text so that it's a bit easier to explore. \n",
    "\n",
    "**Q**: Look at a few of the reviews stored in $\\texttt{text_train}$ as well as their associated labels in $\\texttt{labels_train}$.  Can you figure out which label refers to a positive review and which refers to a negative review? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first review is labeled $1$ and has the following text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth review is labeled $0$ and has the following text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully it's obvious that label 1 corresponds to positive reviews and label 0 to negative reviews! \n",
    "\n",
    "OK, the first thing we'll do is train a logistic regression classifier using the Bag-of-Words model, and see what kind of accuracy we can get.  To get started, we need to vectorize the text into mathematical features that we can use.  We'll use CountVectorizer to do the job. (Before starting, I'm going to reload the data and remove the stop words this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, labels_train = read_and_clean_data('data/labeledTrainData.tsv', remove_stops=True)\n",
    "text_test, labels_test = read_and_clean_data('data/labeledTestData.tsv', remove_stops=True)\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "X_bw_train = cvec.fit_transform(text_train)\n",
    "y_train = np.array(labels_train)\n",
    "X_bw_test  = cvec.transform(text_test)\n",
    "y_test  = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How many different words are in the vocabulary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bw_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: It looks like around 17,800 distinct words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we'll train a logistic regression classifier on the training set, and test the accuracy on the test set.  To do this we'll need to load some kind of accuracy metric from sklearn.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "bwLR = LogisticRegression()\n",
    "bwLR.fit(X_bw_train, y_train)\n",
    "pred_bwLR = bwLR.predict(X_bw_test)\n",
    "\n",
    "print \"Logistic Regression accuracy with Bag-of-Words: \", accuracy_score(y_test, pred_bwLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we got an accuracy of around 81% using Bag-of-Words.  Now lets do the same tests but this time with tf-idf features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "X_tf_train = tvec.fit_transform(text_train)\n",
    "X_tf_test  = tvec.transform(text_test)\n",
    "\n",
    "tfLR = LogisticRegression()\n",
    "tfLR.fit(X_tf_train, y_train)\n",
    "pred_tfLR = tfLR.predict(X_tf_test)\n",
    "\n",
    "print \"Logistic Regression accuracy with tf-idf: \", accuracy_score(y_test, pred_tfLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOOHOO**!  With tf-idf features we got around 85% accuracy, which is a 4% improvement. (If you're scoffing at this, wait until you get some more experience working with real-world data.  4% improvement is pretty awesome).  \n",
    "\n",
    "**Q**: Which words are the strongest predictors for a positive review and which words are the strongest predictors for negative reviews? I'm not going to give you the answer to this one because it's the same question we'll ask on the next homework assignment. But if you figure this out you'll have a great head start! \n",
    "\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
